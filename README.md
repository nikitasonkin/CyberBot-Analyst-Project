# Project Overview
Analysis of data from the [CyberNewsBot](https://github.com/nikitasonkin/CyberNewsBot).
This repository contains SQL schema that creates dedicated tables and Jupyter Notebooks that demonstrate how to process and analyze data efficiently. Below is a summary and explanation of each file in the repository.

## Table of Contents
- [Project Overview](#project-overview)
- [create_new_tables.sql](#create_new_tables)
- [DataAnalystCyberProject Notebook](#analysis-notebook)
- [Key Sections](#key-sections)
- [Outputs](#outputs)
- [JSON Files](#json-files)
  - [posted_news_ud.json](#posted-news-udjson)
  - [skipped_news_ud.json](#skipped-news-udjson)
- [Dependencies](#dependencies)
- [Author](#author)

---

<a name="create_new_tables"></a>
#### ðŸ“„ File: [`create_new_tables.sql`](https://github.com/nikitasonkin/Data-Analyst-Project/blob/main/sql/create_new_tables.sql)

This repository defines five core tables for storing and analyzing news articles, topics and trends. Each table is carefully structured with primary keys, auto-increment IDs, appropriate data types and foreign key constraints to ensure data integrity and optimal query performance.

1. **PostedNews**  
   - **Purpose**: Stores all successfully posted news articles along with full metadata.  
   - **Key columns**:  
     - `ID INT IDENTITY(1,1) PRIMARY KEY` â€“ auto-incrementing unique identifier  
     - `text_hash NVARCHAR(64) UNIQUE` â€“ dedupe key for content  
     - `published_date DATE`, `published_time TIME` â€“ publication timestamp  
     - `rss_source NVARCHAR(100)` â€“ geographic/source channel  
     - `keywords NVARCHAR(MAX)` â€“ raw keyword list (nullable)  

2. **SkippedNews**  
   - **Purpose**: Logs all articles that failed to post, including failure reasons and retry counts.  
   - **Key columns**:  
     - `ID INT IDENTITY(1,1) PRIMARY KEY` â€“ auto-incrementing unique identifier  
     - `text_hash NVARCHAR(64) UNIQUE` â€“ prevents duplicate skip entries  
     - `reason NVARCHAR(300)` â€“ explanation for rejection  
     - `fail_count INT` â€“ number of retry attempts  

3. **Topics**  
   - **Purpose**: Summarizes each clustered topic (connected component) for trend analysis.  
   - **Key columns**:  
     - `topic_id INT PRIMARY KEY` â€“ cluster identifier from DBSCAN  
     - `short_title NVARCHAR(500)` â€“ representative title  
     - `num_articles INT`, `num_days INT` â€“ size and duration of the topic  
     - `first_date`, `last_date` â€“ date range of the topic  
     - `main_country NVARCHAR(100)` â€“ most frequent `rss_source`  
     - `top_keywords NVARCHAR(MAX)` â€“ comma-separated top-5 keywords  
     - `spike_detected BIT DEFAULT 0`, `growth_detected BIT DEFAULT 0` â€“ trend flags  

4. **Articles**  
   - **Purpose**: Stores individual articles linked to topics for detailed, article-level analysis.  
   - **Key columns**:  
     - `article_id INT IDENTITY(1,1) PRIMARY KEY` â€“ auto-incrementing ID  
     - `topic_id INT FOREIGN KEY REFERENCES Topics(topic_id)` â€“ topic association  
     - `similar_count INT` â€“ number of peer articles in the same topic  
     - `cleaned_keywords NVARCHAR(MAX)` â€“ normalized keyword list  

5. **Trends**  
   - **Purpose**: Captures the most important topics for reporting and dashboards.  
   - **Key columns**:  
     - `topic_id INT PRIMARY KEY REFERENCES Topics(topic_id)` â€“ identifies the trend topic  
     - `representative_summary NVARCHAR(MAX)` â€“ text summary of the trend  
     - (inherits all summary fields from `Topics` for easy dashboarding)  


---

<a name="analysis-notebook"></a>
#### ðŸ“„ File: [`DataAnalystCyberProject.ipynb`](https://github.com/nikitasonkin/Data-Analyst-Project/blob/main/notebooks/DataAnalystCyberProject.ipynb)

A single, end-to-end Python script that drives the CyberNewsBot analytics pipelineâ€”from JSON ingestion and SQL ETL, through exploratory data checks, feature engineering, semantic clustering and topic/trend extraction, to a comprehensive suite of publication-ready visualizations.

<a name="key-sections"></a>
##### ðŸ”¹ Key Sections

1. **Imports & SQLAlchemy Setup**  
   - Standard, pandas/numpy, SQLAlchemy, matplotlib/seaborn, NLP & ML libraries  
   - Defines `server`, `database`, `driver` and builds the `engine`

2. **ETL Functions**  
   - `load_posted_news(engine, json_path)`  
   - `load_skipped_news(engine, json_path)`  
   - Read JSON â†’ dedupe on `text_hash` â†’ return new entries

3. **Main Entry Point**  
   - `main()` invokes both loaders  
   - Appends new records to `PostedNews` and `SkippedNews`  
   - Prints summary counts and sample titles

4. **Data Retrieval & EDA**  
   - Reads `PostedNews` and `SkippedNews` back into DataFrames  
   - Prints shape, duplicate counts, null counts, `.info()`, `.describe()`, random samples, per-date frequency

5. **Temporal Feature Engineering**  
   - Parses `published_date`/`published_time` â†’ datetime, `.dt.day_name()`, `.dt.hour`  
   - Builds side-by-side `comparison_df` of daily Sent vs Skipped

6. **Keyword Cleaning & Top-N Extraction**  
   - `clean_keywords()` to normalize & filter stopwords  
   - `get_top_keywords()` â†’ DataFrame of top keywords  
   - `plot_top_keywords_pie()` â†’ pie (or donut) chart of keyword share

7. **Publication & Failure Trends**  
   - Daily lineplots of Published vs Failed articles  
   - Growth rate area chart with 7-day MA  
   - Bar-chart of failure reasons (percentage)

8. **Country-Level Breakdown**  
   - Side-by-side barplot of Published vs Skipped by `rss_source`

9. **Heatmap of Failures**  
   - `sns.heatmap()` of failure counts by date Ã— reason

10. **Duplicate Analysis Helpers**  
    - `show_top_dupes()` for title, hash, and URL duplicates  
    - Prints top N with counts and percentages

11. **Hourly Flow Scatter**  
    - `build_hourly_tables()` + `plot_hourly_flow()` for 7-day scatter of published vs duplicates

12. **Semantic Embedding & Clustering**  
    - SentenceTransformer embeddings of `title + summary`  
    - DBSCAN clustering (cosine metric) â†’ `topic_id`, `similar_count`

13. **Topic Filtering & Aggregation**  
    - Identify `large_topics` (â‰¥5 articles)  
    - Build `agg_df` and `topic_summary` for examples

14. **Trend Classification & SQL Write-Back**  
    - Build `topics_df` with counts, date ranges, top keywords, `spike`/`growth` flags  
    - Persist new Topics, Articles, Trends into SQL tables  

15. **Top-N Trending Topics Visualization**  
    - Barplot of top 10 by `num_articles`  
    - Daily line/area chart for top topics  
    - WordCloud of trending keywords  
    - Horizontal bar chart with representative summaries and metadata

<a name="outputs"></a>
##### ðŸ“Š Outputs

- **Console**: ETL summaries, EDA statistics, duplicate and topic/trend counts  
- **Charts**: scatterflows, time-series & area plots, pie/donut, bar/heatmap, wordcloud, trend dashboards  

---

<a name="json-files"></a>
##### ðŸ“‚JSON Files: DATA/`posted_news_ud.json` & DATA/`skipped_news_ud.json`

These files are automatically generated and updated by the **CyberNewsBot** system. They serve as the raw data sources for all analysis and trend detection processes.

<a name="posted-news-udjson"></a>
##### `posted_news_ud.json`

Contains all cybersecurity news articles that were successfully published and sent by the bot.  
Each article includes the following fields:

- `title` â€“ the article's title  
- `url` â€“ direct link to the article  
- `summary` â€“ short summary of the content  
- `source` â€“ publication source (e.g., website name)  
- `published_date` â€“ date of publication (YYYY-MM-DD)  
- `published_time` â€“ time of publication (HH:MM:SS)  
- `rss_source` â€“ geographic source (country/channel)  
- `keywords` â€“ extracted keywords from the content  
- `text_hash` â€“ unique identifier for duplicate detection  

<a name="skipped-news-udjson"></a>
##### `skipped_news_ud.json`

Stores all articles that were **rejected** during processing due to filters or errors.  
Each skipped entry includes all base fields from `posted_news_ud.json`, plus:

- `reason` â€“ explanation for rejection (e.g., duplicate title, short summary)  
- `fail_count` â€“ how many times the article was skipped  
- `date` â€“ timestamp when the rejection was logged  

---
<a name="dependencies"></a>
##### Dependencies

- **Standard Library**  
  - `json`  
  - `warnings`  

- **Data Manipulation**  
  - `pandas`  
  - `numpy`  

- **Database Connectivity**  
  - `sqlalchemy`  
    - `create_engine`  
    - `NVARCHAR`, `Date`, `Time`  

- **Visualization**  
  - `matplotlib`  
    - `pyplot`  
    - `colors` (`mcolors`)  
    - `cm`  
    - `dates` (`DateFormatter`, `mdates`)  
    - `Line2D`  
    - `LogNorm`  
  - `seaborn`  
  - `wordcloud` (`WordCloud`)  

- **Graph Analysis**  
  - `networkx`  

- **Machine Learning / NLP**  
  - `scikit-learn`  
    - `TfidfVectorizer`  
    - `CountVectorizer`  
    - `KMeans`  
    - `DBSCAN`  
  - `sentence-transformers`  
    - `SentenceTransformer`  
    - `util`  

---

### License
-This project is licensed under the [MIT License](LICENSE).

<a name="author"></a>
### Author
- **Created by:** Nikita Sonkin  
- **Project repository:** [CyberNewsBot on GitHub](https://github.com/nikitasonkin/CyberNewsBot)
