{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b4fae1-a742-436a-8c4a-f98b2eb57abe",
   "metadata": {},
   "source": [
    "#### Creating a connection to SQL Server using SQLAlchemy and ODBC, to allow loading JSON tables into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c75d3f-ea69-437a-ad3c-7e381b19938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.2.2-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\nikita\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Downloading sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.2-cp313-cp313-win_amd64.whl (296 kB)\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.2.2 sqlalchemy-2.0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Nikita\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0768ef7-5663-4ff3-81f2-5be1a0df6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835bd43f-0965-4272-9474-a21b6668a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Server connection settings\n",
    "server = 'NIKITA-PC'\n",
    "database = 'CyberNewsBot'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "connection_string = (\n",
    "    f\"mssql+pyodbc://@{server}/{database}\"\n",
    "    f\"?driver={driver.replace(' ', '+')}&Trusted_Connection=yes\"\n",
    ")\n",
    "engine = create_engine(connection_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a81fc-5f2a-4497-8c4c-d9eb3f7da7a9",
   "metadata": {},
   "source": [
    "### üîπ Step 1: Loading Posted News json file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae56d8-53e5-4f89-bc00-dc5b4bb13889",
   "metadata": {},
   "source": [
    "#####     1.Load the posted_news_ud.json file.\n",
    "#####     2.Convert the keywords column to a string.\n",
    "#####     3.Check if an article already exists by text_hash.\n",
    "#####     4.Load only the latest news into the PostedNews table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3143ef-55e2-4b84-a7c0-cec92d9ff7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 90 new articles to PostedNews.\n"
     ]
    }
   ],
   "source": [
    "# Reading from JSON file\n",
    "posted_df = pd.read_json(r\"posted_news_ud.json\")\n",
    "\n",
    "# Convert keywords to a comma-separated string list\n",
    "if 'keywords' in posted_df.columns:\n",
    "    posted_df['keywords'] = posted_df['keywords'].apply(\n",
    "        lambda x: ','.join(x) if isinstance(x, list) else str(x)\n",
    "    )\n",
    "\n",
    "#  Filter duplicates by text_hash\n",
    "existing_hashes = pd.read_sql(\"SELECT text_hash FROM PostedNews\", con=engine)\n",
    "new_posted_df = posted_df[~posted_df['text_hash'].isin(set(existing_hashes['text_hash']))]\n",
    "\n",
    "# Loading into the database\n",
    "if not new_posted_df.empty:\n",
    "    new_posted_df.to_sql(\"PostedNews\", con=engine, if_exists=\"append\", index=False, chunksize=1000)\n",
    "    print(f\"‚úÖ Loaded {len(new_posted_df)} new articles to PostedNews.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No new articles to load into PostedNews.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130870c-606b-4f42-b139-bffcb4629575",
   "metadata": {},
   "source": [
    "### üîπ Step 2: Loading SkippedNews json file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd46de8-dfbc-421b-b3b3-46854fc2c79c",
   "metadata": {},
   "source": [
    "#####     1.Load the dictionary from skipped_news_ud.json.\n",
    "#####     2.Check if an article already exists by text_hash\n",
    "#####     3.Load only the latest news into the SkippedNews table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a04b0032-6463-4ed8-820d-2c5d0df98fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 116 new articles to SkippedNews.\n"
     ]
    }
   ],
   "source": [
    "# Reading from JSON file\n",
    "with open(r\"skipped_news_ud.json\", encoding='utf-8') as f:\n",
    "    skipped_data = list(json.load(f).values())\n",
    "skipped_df = pd.DataFrame(skipped_data)\n",
    "\n",
    "#  Filter duplicates by text_hash\n",
    "existing_skipped = pd.read_sql(\"SELECT text_hash FROM SkippedNews\", con=engine)\n",
    "new_skipped_df = skipped_df[~skipped_df['text_hash'].isin(set(existing_skipped['text_hash']))]\n",
    "new_skipped_df = new_skipped_df.drop_duplicates(subset='text_hash')\n",
    "\n",
    "# Loading into the database\n",
    "if not new_skipped_df.empty:\n",
    "    new_skipped_df.to_sql(\"SkippedNews\", con=engine, if_exists=\"append\", index=False, chunksize=1000)\n",
    "    print(f\"Loaded {len(new_skipped_df)} new articles to SkippedNews.\")\n",
    "else:\n",
    "    print(\"No new articles to load into SkippedNews.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb5fc7-8f53-4664-a4ce-dfdefba5c38a",
   "metadata": {},
   "source": [
    "### Step 3: General tests\n",
    "#Print some examples of the failed articles and perform a Missing Values ‚Äã‚Äãcheck to monitor data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593d8a45-cf64-41a2-898e-62fc731facf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample of newly skipped articles:\n",
      "                                                  title  \\\n",
      "1497  Markets/Coverages: Markel Launches Affirmative...   \n",
      "1498  Marks and Spencer could face 12% drop in profi...   \n",
      "1499  AI risks creating a cyber crunch for the UK's ...   \n",
      "\n",
      "                                  reason  \n",
      "1497           Duplicate by summary hash  \n",
      "1498  Article text is empty or too short  \n",
      "1499                  Duplicate by title  \n",
      "\n",
      " Missing value check (SkippedNews):\n",
      "title             0\n",
      "url               0\n",
      "fail_count        0\n",
      "date              0\n",
      "reason            0\n",
      "text_hash         0\n",
      "summary           0\n",
      "source            0\n",
      "published_date    0\n",
      "published_time    0\n",
      "rss_source        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Sample of newly skipped articles:\")\n",
    "print(new_skipped_df[['title', 'reason']].head(3))\n",
    "\n",
    "print(\"\\n Missing value check (SkippedNews):\")\n",
    "print(new_skipped_df.isnull().sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
