# ===========================================================================================
# ğŸ”¹ 1. Imports and Configuration
# ===========================================================================================
#  Data handling and analysis
import pandas as pd                            # × ×™×ª×•×— ×•×¢×™×‘×•×“ ×˜×‘×œ××•×ª × ×ª×•× ×™×
import numpy as np                             # ×—×™×©×•×‘×™× ××¡×¤×¨×™×™× ×•××˜×¨×™×¦×•×ª
#  Visualization
import matplotlib.pyplot as plt                # ×¦×™×•×¨ ×’×¨×¤×™×
import seaborn as sns                          # ×•×™×–×•××œ×™×–×¦×™×” ××ª×§×“××ª ×¢×œ ×‘×¡×™×¡ matplotlib
from wordcloud import WordCloud                # ×™×¦×™×¨×ª ×¢× × ×™ ××™×œ×™×
#  Machine Learning / NLP
from sklearn.feature_extraction.text import TfidfVectorizer  # ×”××¨×ª ×˜×§×¡×˜×™× ×œ××˜×¨×™×¦×ª TF-IDF
from sklearn.feature_extraction.text import CountVectorizer  # ×”××¨×ª ×˜×§×¡×˜×™× ×œ××˜×¨×™×¦×ª ×¡×¤×™×¨×”
from sklearn.cluster import KMeans                          # ××œ×’×•×¨×™×ª× ×œ×§×™×‘×•×¥ (Clustering)
from sentence_transformers import SentenceTransformer       # ×”×¤×§×ª embeddings ×œ××©×¤×˜×™×
from sentence_transformers import util                      # ×—×™×©×•×‘ cosine similarity ×‘×™×Ÿ embeddings
#  Database
from sqlalchemy import create_engine          # ×—×™×‘×•×¨ ×•×©××™×¨×” ×œ××¡×“×™ × ×ª×•× ×™× (SQL)
#  Graph & Clustering
import networkx as nx                         # ×¢×‘×•×“×” ×¢× ×’×¨×¤×™×, ××¦×™××ª ×¨×›×™×‘×™× ××—×•×‘×¨×™×

# ===========================================================================================
# ğŸ”¹ 2. loading data
# ===========================================================================================
# ×”×’×“×¨×•×ª ×—×™×‘×•×¨ ×œ-SQL
server = 'NIKITA-PC'
database = 'CyberNewsBot'
driver = 'ODBC Driver 17 for SQL Server'

connection_string = f"mssql+pyodbc://@{server}/{database}?driver={driver.replace(' ', '+')}&Trusted_Connection=yes"
engine = create_engine(connection_string)

# ×©×œ×™×¤×ª ×˜×‘×œ××•×ª
posted_df = pd.read_sql("SELECT * FROM PostedNews", con=engine)
skipped_df = pd.read_sql("SELECT * FROM SkippedNews", con=engine)

# ×¢××•×“×•×ª ID ×¨×¥
posted_df['ID'] = posted_df.index
skipped_df['ID'] = skipped_df.index
# ===========================================================================================
# ğŸ”¹ 3. Initial review
# ===========================================================================================
print("============================================ ×¡×§×™×¨×” ×¨××©×•× ×™×ª ===============================================")

print("=== POSTED DF SHAPE ===", posted_df.shape)
print("=== POSTED DUPLICATES ===", posted_df.duplicated().sum())
print("=== POSTED COLUMNS ===", posted_df.columns.tolist())
print("=== POSTED NULLS ===\n", posted_df.isnull().sum())
print("=== POSTED INFO ===")
posted_df.info()
print("=== POSTED DESCRIBE ===\n", posted_df.describe(include='all'))
print("\n××¡×¤×¨ ×¢×¨×›×™× ×™×™×—×•×“×™×™× ×œ×›×œ ×¢××•×“×”:")
print(posted_df.nunique())
print("\n ×“×’×™××” ××§×¨××™×ª ×-posted_df:")
print(posted_df.sample(5))
print("\n ×¤×™×œ×•×— ×›×ª×‘×•×ª ×œ×¤×™ ×ª××¨×™×š:")
print(posted_df['published_date'].value_counts().sort_index())
print("-----------------------------------------------------------------------------------")
print("=== SKIPPED DF SHAPE ===", skipped_df.shape)
print("=== SKIPPED DUPLICATES ===", skipped_df.duplicated().sum())
print("=== SKIPPED COLUMNS ===", skipped_df.columns.tolist())
print("=== SKIPPED NULLS ===\n", skipped_df.isnull().sum())
print("=== SKIPPED INFO ===")
skipped_df.info()
print("=== SKIPPED DESCRIBE ===\n", skipped_df.describe(include='all'))
print("\n ××¡×¤×¨ ×¢×¨×›×™× ×™×™×—×•×“×™×™× ×œ×›×œ ×¢××•×“×”:")
print(skipped_df.nunique())
print("\n ×“×’×™××” ××§×¨××™×ª ×-skipped_df:")
print(skipped_df.sample(5))
print("\n ×¤×™×œ×•×— ×›×ª×‘×•×ª ×œ×¤×™ ×ª××¨×™×š:")
print(skipped_df['published_date'].value_counts().sort_index())

# ===========================================================================================
#  ğŸ”¹4. Date conversion + auxiliary columns
# ===========================================================================================
print("============================================ ×”××¨×ª ×ª××¨×™×›×™× + ×¢××•×“×•×ª ×¢×–×¨ ===============================================")

# ×”××¨×ª ×¢××•×“×•×ª ×ª××¨×™×š ×œ×–××Ÿ ×ª×§× ×™ (Datetime / Time)
posted_df['published_date'] = pd.to_datetime(posted_df['published_date'], errors='coerce')
posted_df['published_time'] = pd.to_datetime(posted_df['published_time'], format='%H:%M:%S', errors='coerce').dt.time

skipped_df['published_date'] = pd.to_datetime(skipped_df['published_date'], errors='coerce')
skipped_df['published_time'] = pd.to_datetime(skipped_df['published_time'], format='%H:%M:%S', errors='coerce').dt.time

# ×™×¦×™×¨×ª ×¢××•×“×•×ª ×¢×–×¨
posted_df['day_of_week'] = posted_df['published_date'].dt.day_name()
posted_df['hour'] = pd.to_datetime(posted_df['published_time'], errors='coerce').apply(lambda x: x.hour if pd.notnull(x) else np.nan)

skipped_df['day_of_week'] = skipped_df['published_date'].dt.day_name()
skipped_df['hour'] = pd.to_datetime(skipped_df['published_time'], errors='coerce').apply(lambda x: x.hour if pd.notnull(x) else np.nan)

print(" ×”××¨×ª ×ª××¨×™×›×™× ×•×”×•×¡×¤×ª ×¢××•×“×•×ª ×¢×–×¨ ×”×•×©×œ××”.")

# ===========================================================================================
# ğŸ”¹ 5. Statistical groupings and basic graphs
# ===========================================================================================
print("============================================ ×§×™×‘×•×¦×™× ×¡×˜×˜×™×¡×˜×™×™× ×•×’×¨×¤×™× ×‘×¡×™×¡×™×™× ===============================================")

# ×¡×˜×•×¤ ×•×•×¨×“×¡ ×•× ×™×§×•×™ ××™×œ×•×ª ××¤×ª×—
stopwords = {"the", "a", "an", "and", "or", "with", "by", "from", "after", "against", "company", "their", "its", "on",
             "for", "in", "of", "to", "is", "are", "was", "as", "this", "that", "these", "those", "about", "into",
             "platform", "portal", "manage", "known", "primarily", "better", "current", "systems"}

def clean_keywords(keyword_str):
    return [
        word.strip().lower()
        for word in str(keyword_str).split(',')
        if word.strip().lower() and word.strip().lower() not in stopwords
    ]
# ×™×¦×™×¨×ª ×¢××•×“×ª cleaned_keywords
posted_df['cleaned_keywords'] = posted_df['keywords'].apply(clean_keywords)

# -------------------------------------------------------------------------------------------
# ğŸ”¹General statistics about the data
# -------------------------------------------------------------------------------------------

# ×××•×¦×¢ ××™×œ×•×ª ××¤×ª×— ×œ×›×ª×‘×”
avg_keywords_per_article = posted_df['cleaned_keywords'].apply(len).mean()
print(f" ×××•×¦×¢ ××¡×¤×¨ ××™×œ×•×ª ××¤×ª×— ×œ×›×ª×‘×”: {avg_keywords_per_article:.2f}")

# ×¤×™×¨×•×§ keywords ×œ×©×•×¨×•×ª
exploded = posted_df.explode('cleaned_keywords')

# ×”×“×¤×¡×ª ××™×œ×•×ª ××¤×ª×— ×©××•×¤×™×¢×•×ª ×œ×¤×—×•×ª 10 ×¤×¢××™×
print(" ××™×œ×•×ª ××¤×ª×— ×¤×•×¤×•×œ×¨×™×•×ª (×œ×¤×—×•×ª 10 ××•×¤×¢×™×):")
for kw, count in exploded['cleaned_keywords'].value_counts().items():
    if count >= 10:
        print(f"{kw} - {count}")

# -------------------------------------------------------------------------------------------
# ğŸ”¹Statistical aggregations
# -------------------------------------------------------------------------------------------

# ×¡×š ×›×ª×‘×•×ª ×©×¤×•×¨×¡××• ×œ×¤×™ ×ª××¨×™×š
articles_per_day = posted_df.groupby('published_date').size()
# ×¡×š ×›×ª×‘×•×ª ×©× ×›×©×œ×• ×œ×¤×™ ×ª××¨×™×š
failures_per_day = skipped_df.groupby('published_date').size()
# ×¡×š ×›×ª×‘×•×ª ×¤×•×¨×¡××•×ª ×œ×¤×™ ××“×™× ×”
articles_per_country = posted_df['rss_source'].value_counts()
# ×¡×™×‘×•×ª ×›×™×©×œ×•×Ÿ ×œ×¤×™ ×ª××¨×™×š
failures_by_reason_date = skipped_df.groupby(['published_date', 'reason']).size().unstack(fill_value=0)

# -------------------------------------------------------------------------------------------
# ğŸ”¹Graph 1: Number of articles published by date
# -------------------------------------------------------------------------------------------

plt.figure(figsize=(14,7))
articles_per_day.plot(kind='line', marker='o')
plt.title('Number of Published Articles Per Day')
plt.xlabel('Published Date')
plt.ylabel('Number of Articles')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# -------------------------------------------------------------------------------------------
# ğŸ”¹Graph 2: Daily number of failures
# -------------------------------------------------------------------------------------------

plt.figure(figsize=(14,7))
failures_per_day.plot(kind='line', color='red', marker='o')
plt.title('Number of Failed Articles Per Day')
plt.xlabel('Published Date')
plt.ylabel('Number of Failed Articles')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# -------------------------------------------------------------------------------------------
# ğŸ”¹Graph 3: Segmentation of failure reasons
# -------------------------------------------------------------------------------------------

plt.figure(figsize=(10, 6))
skipped_df['reason'].value_counts().plot(kind='bar', color='tomato')
plt.title('Failure Reasons Distribution')
plt.xlabel('Reason')
plt.ylabel('Number of Failures')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# -------------------------------------------------------------------------------------------
# ğŸ”¹Graph 4: Segmentation of articles by country
# -------------------------------------------------------------------------------------------

plt.figure(figsize=(12,6))
articles_per_country.plot(kind='bar')
plt.title('Number of Published Articles by Country')
plt.xlabel('Country')
plt.ylabel('Number of Articles')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# -------------------------------------------------------------------------------------------
#  ğŸ”¹Graph 5: Heatmap â€“ Failure Reason by Date
# -------------------------------------------------------------------------------------------

failures_by_reason_date.index = failures_by_reason_date.index.strftime('%Y-%m-%d')
plt.figure(figsize=(14,7))
sns.heatmap(failures_by_reason_date, annot=True, fmt='d', cmap='YlOrBr')
plt.title('Failures by Reason and Date')
plt.xlabel('Failure Reason')
plt.ylabel('Published Date')
plt.tight_layout()
plt.show()

# ğŸŸ¢ ×¡×™×•× ×©×œ×‘ ×’×¨×¤×™× ×¡×˜×˜×™×¡×˜×™×™×
print(" ×’×¨×¤×™× ×¡×˜×˜×™×¡×˜×™×™× ×‘×¡×™×¡×™×™× × ×•×¦×¨×• ×‘×”×¦×œ×—×”.")

# ===========================================================================================
# ï¸ğŸ”¹ Step 6: Identify and compare duplicates between published and rejected articles (title + summary hash)
# ===========================================================================================
print("============================================ ×–×™×”×•×™ ×•×”×©×•×•××ª ×›×¤×™×œ×•×™×•×ª ×‘×™×Ÿ ×›×ª×‘×•×ª ×©×¤×•×¨×¡××• ×•× ×“×—×• ===============================================")

#  ×¡×•×¤×¨×™× ×›××” ×¤×¢××™× ×”×•×¤×™×¢×• ×›×•×ª×¨×™× ×©×”×•×‘×™×œ×• ×œ×“×—×™×™×” (Duplicate by title)
title_counts = skipped_df[skipped_df['reason'] == 'Duplicate by title'].groupby('title').size()

#  ×¡×•×¤×¨×™× ×›××” ×¤×¢××™× ×”×•×¤×™×¢ ×ª×§×¦×™×¨ ×–×”×” (Duplicate by summary hash)
hash_counts = skipped_df[skipped_df['reason'] == 'Duplicate by summary hash'].groupby('text_hash').size()

#  ×”×•×¡×¤×ª ×¢××•×“×•×ª ×œ-posted_df: ×›××” ×›×¤×™×œ×•×™×•×ª ×›×œ ×›×ª×‘×” ×’×¨××”
posted_df['duplicated_titles_in_skipped'] = posted_df['title'].map(title_counts).fillna(0).astype(int)
posted_df['duplicated_hashes_in_skipped'] = posted_df['text_hash'].map(hash_counts).fillna(0).astype(int)

# ×—×™×©×•×‘ ×›×•×œ×œ ×©×œ ×›×¤×™×œ×•×™×•×ª ×©×›×ª×‘×” ××—×ª ×’×¨××” ×œ×”×Ÿ
posted_df['total_duplicates_caused'] = posted_df['duplicated_titles_in_skipped'] + posted_df['duplicated_hashes_in_skipped']

# -------------------------------------------------------------------------------------------
#  ğŸ”¹Printing the most "noisy" articles that caused the most duplications
# -------------------------------------------------------------------------------------------

print("\n=== ×“×•×’×××•×ª ×©×œ ×›×ª×‘×•×ª ×¢× ×”×›×™ ×”×¨×‘×” ×›×¤×™×œ×•×™×•×ª ×‘×›×•×ª×¨×ª (Top 10) ===")
print(posted_df[['title', 'duplicated_titles_in_skipped']].sort_values('duplicated_titles_in_skipped', ascending=False).head(10))

print("\n=== ×“×•×’×××•×ª ×©×œ ×›×ª×‘×•×ª ×¢× ×”×›×™ ×”×¨×‘×” ×›×¤×™×œ×•×™×•×ª ×‘×ª×§×¦×™×¨ (Top 10) ===")
print(posted_df[['title', 'duplicated_hashes_in_skipped']].sort_values('duplicated_hashes_in_skipped', ascending=False).head(10))

# -------------------------------------------------------------------------------------------
#  ğŸ”¹Graph: General distribution of the number of duplicates by article
# -------------------------------------------------------------------------------------------

plt.figure(figsize=(12, 6))
plt.hist(
    posted_df['total_duplicates_caused'],
    bins=range(0, posted_df['total_duplicates_caused'].max() + 2),
    edgecolor='black'
)
plt.title('Distribution of Total Duplicates Caused by Published Articles')
plt.xlabel('Number of Duplicated Articles Caused')
plt.ylabel('Number of Published Articles')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

print(" ×—×™×©×•×‘ ×›×¤×™×œ×•×™×•×ª ×•×™×¦×™×¨×ª ×’×¨×£ ×”×ª×¤×œ×’×•×ª ×‘×•×¦×¢×• ×‘×”×¦×œ×—×”.")

# ===========================================================================================
# ğŸ”¹ Step 7: Identify similar articles using cosine similarity on title + summary
# ===========================================================================================
print("============================================ ×–×™×”×•×™ ×›×ª×‘×•×ª ×“×•××•×ª (cosine similarity) ===============================================")

# ×©×™×œ×•×‘ ×›×•×ª×¨×ª + ×ª×§×¦×™×¨ ×œ×˜×§×¡×˜ ××œ× ×œ×›×œ ×›×ª×‘×”
posted_df['super_text'] = posted_df['title'].astype(str) + " " + posted_df['summary'].astype(str)

# ×”××¨×ª ×˜×§×¡×˜ ×œÖ¾embeddings
model = SentenceTransformer('paraphrase-MiniLM-L6-v2') #×˜×¢×™× ×ª ×”××•×“×œ ×–×”×• ××•×“×œ ×××•××Ÿ ××¨××© ×©×™×•×“×¢ ×œ×”××™×¨ ××©×¤×˜×™× ×œ×•×•×§×˜×•×¨×™× (vectors) ×©××™×™×¦×’×™× ××ª ×”××©××¢×•×ª ×©×œ ×”××©×¤×˜.
embeddings = model.encode(posted_df['super_text'].tolist(), show_progress_bar=True) # ×”××¨×ª ×˜×§×¡×˜ ×œÖ¾embeddings ××©×ª× ×” embeddings ××›×™×œ ××¢×¨×š ×‘×’×•×“×œ (××¡×¤×¨ ×›×ª×‘×•×ª, ×’×•×“×œ embedding)

print("\n=== ×–×™×”×•×™ ×›×ª×‘×•×ª ×“×•××•×ª ×œ×¤×™ cosine similarity ===")

cosine_sim_matrix = util.cos_sim(embeddings, embeddings).cpu().numpy() #×—×™×©×•×‘ ××˜×¨×™×¦×ª ×“××™×•×Ÿ ×§×•×¡×™× ×™

# ===========================================================================================
#  ğŸ”¹Threshold for filtering and mapping similar articles by similarity level > 0.8
# ===========================================================================================

threshold = 0.8 # ×–×™×”×•×™ ×›×ª×‘×•×ª ×¢× ×“××™×•×Ÿ ×’×‘×•×” ×Ö¾0.8 (×œ×œ× ×–×”×•×ª ×¢×¦××™×ª)
similar_articles_dict = {} #×™×•×¦×¨×™× ××™×œ×•×Ÿ ×©×™××—×¡×Ÿ ×¢×‘×•×¨ ×›×œ ×›×ª×‘×” ××ª ×”×¨×©×™××” ×©×œ ×›×ª×‘×•×ª ×©×“×•××•×ª ×œ×” ×‘×¨××ª ×¡×£ ×©× ×§×‘×¢.

for i in range(len(posted_df)):
    similar_indices = np.where((cosine_sim_matrix[i] >= threshold) & (cosine_sim_matrix[i] < 0.999))[0]
    if len(similar_indices) > 0:
        similar_articles_dict[i] = list(similar_indices)

# ×¢××•×“×ª ×›×ª×‘×•×ª ×“×•××•×ª
posted_df['similar_articles'] = posted_df.index.map(similar_articles_dict)
# ×¢××•×“×ª ××•× ×” ×›×ª×‘×•×ª ×“×•××•×ª
posted_df['similar_count'] = posted_df['similar_articles'].apply(lambda x: len(x) if isinstance(x, list) else 0)#×× x ×”×•× ×¨×©×™××” â†’ ××—×–×™×¨×” ××ª ×”××•×¨×š ×©×œ×” (×›×œ×•××¨: ×›××” ×›×ª×‘×•×ª ×“×•××•×ª ×™×©).

# ===========================================================================================
#  ğŸ”¹View the top 10 most "repeated" articles by ideal similarity
# ===========================================================================================

print("\n ×›×ª×‘×•×ª ×¢× ×”×›×™ ×”×¨×‘×” ×“××™×•×Ÿ (cosine > 0.8):")
print(posted_df[['ID', 'title', 'similar_count']].sort_values('similar_count', ascending=False).head(10))

# ×’×¨×£ ×”×ª×¤×œ×’×•×ª: ×›××” ×›×ª×‘×•×ª ×“×•××•×ª ×™×© ×œ×›×œ ×›×ª×‘×”
plt.figure(figsize=(10, 6))
plt.hist(posted_df['similar_count'], bins=range(0, posted_df['similar_count'].max() + 2), edgecolor='black')
plt.title('Distribution of Similar Articles per Article (cosine > 0.8)')
plt.xlabel('Number of Similar Articles')
plt.ylabel('Number of Articles')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

print(" ×–×™×”×•×™ ×›×ª×‘×•×ª ×“×•××•×ª ×”×•×©×œ× ×‘×”×¦×œ×—×”.")


# ===========================================================================================
# ğŸ”¹ Step 8: Building topic clusters (Topics) based on conceptual similarities between articles
# ===========================================================================================
print("============================================ ×‘× ×™×™×ª ××©×›×•×œ×•×ª × ×•×©× (Topics) ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×¨×¢×™×•× ×™ ×‘×™×Ÿ ×›×ª×‘×•×ª ===============================================")

print(" ×‘× ×™×™×ª ×§×‘×•×¦×•×ª × ×•×©× ×œ×¤×™ cosine similarity...")

# -------------------------------------------------------------------------------------------
#  ğŸ”¹1. Building a graph of connections between similar articles (undirected graph)
# -------------------------------------------------------------------------------------------

# ×’×¨×£ ×§×©×¨×™×
G = nx.Graph() #×™×•×¦×¨ ×’×¨×£ ×‘×œ×ª×™ ××›×•×•×Ÿ
for i, sims in posted_df['similar_articles'].dropna().items():#×–×•×’×•×ª (i, sims), ×›××©×¨ i ×”×•× ××™× ×“×§×¡ ×•×”Ö¾sims ×”× ×¨×©×™××ª ××™× ×“×§×¡×™× ×“×•××™×. ××¡× ×Ÿ ×›×ª×‘×•×ª ×©××™×Ÿ ×œ×”×Ÿ ×“×•××•×ª.
    G.add_node(i) # ××•×¡×™×£ ××ª ×”×›×ª×‘×” ×›×¦×•××ª ×‘×’×¨×£
    for j in sims: # ×¢×‘×•×¨ ×›×œ ×›×ª×‘×” ×“×•××”
        G.add_edge(i, j) # ××•×¡×™×£ ×§×©×ª ×‘×™×Ÿ ×”×›×ª×‘×” ×”× ×•×›×—×™×ª ×œ×›×ª×‘×” ×”×“×•××” ×œ×”

# -------------------------------------------------------------------------------------------
#  ğŸ”¹2. Identifying connected components (article clusters)
# -------------------------------------------------------------------------------------------

connected_groups = list(nx.connected_components(G)) # ××—×–×™×¨ ×¨×©×™××” ×©×œ ×§×‘×•×¦×•×ª ××—×•×‘×¨×•×ª ×‘×’×¨×£. ×›×œ ×§×‘×•×¦×” ×”×™× ××•×¡×£ ×©×œ ×¦××ª×™× (×›×ª×‘×•×ª) ×©××—×•×‘×¨×•×ª ×–×• ×œ×–×•.

# ××™×¤×•×™ ×œÖ¾topic_id
topic_mapping = {} #  ×›×œ ×›×ª×‘×” â†’ ××¡×¤×¨ ××©×›×•×œ
topic_article_ids = {}  # ×›×œ topic_id â†’ ×¨×©×™××ª ×›×ª×‘×•×ª ×‘××©×›×•×œ

for idx, group in enumerate(connected_groups): #enumerate × ×•×ª×Ÿ ××¡×¤×¨ ××–×”×” (idx) ×œ×›×œ ×§×‘×•×¦×ª ×›×ª×‘×•×ª.
    for article_id in group: #group ×–×” ××•×¡×£ ×”××™× ×“×§×¡×™× ×©×œ ×”×›×ª×‘×•×ª ×‘××•×ª×• ××©×›×•×œ.
        topic_mapping[article_id] = idx # ××™×¤×•×™ ×”×›×ª×‘×” ×œ××¡×¤×¨ ×”××©×›×•×œ ×©×œ×”
    topic_article_ids[idx] = list(group) # ××™×¤×•×™ ××¡×¤×¨ ×”××©×›×•×œ ×œ×›×ª×‘×•×ª ×©×œ×•

posted_df['topic_id'] = posted_df['ID'].map(topic_mapping)# ××™×¤×•×™ ×œ×›×œ ×›×ª×‘×” ××ª ××¡×¤×¨ topic_id

# -------------------------------------------------------------------------------------------
#  ğŸ”¹3. Create a topic table with at least 5 articles
# -------------------------------------------------------------------------------------------

# ×˜×‘×œ×ª × ×•×©××™×
topic_df = posted_df[posted_df['topic_id'].notnull()].copy()
topic_df['topic_id'] = topic_df['topic_id'].astype(int)

# × ×•×©××™× ×¢× ×œ×¤×—×•×ª 5 ×›×ª×‘×•×ª
topic_sizes = topic_df['topic_id'].value_counts()
large_topics = topic_sizes[topic_sizes >= 5].index.tolist()
print(f" × ××¦××• {len(large_topics)} × ×•×©××™× ×¢× ×œ×¤×—×•×ª 5 ×›×ª×‘×•×ª")

# -------------------------------------------------------------------------------------------
#  ğŸ”¹4. Building a summary table for topics
# -------------------------------------------------------------------------------------------

topic_summary = pd.DataFrame({
    'topic_id': large_topics ,# ×¨×©×™××ª × ×•×©××™× ×¢× ×œ×¤×—×•×ª 5 ×›×ª×‘×•×ª,
    'num_articles': [topic_sizes[t] for t in large_topics], # ××¡×¤×¨ ×›×ª×‘×•×ª ×‘×›×œ × ×•×©×
    'first_date': [topic_df[topic_df['topic_id'] == t]['published_date'].min().date() for t in large_topics], # ×ª××¨×™×š ×¨××©×•×Ÿ ××—×¤×© ××ª ×”×ª××¨×™×š ×”×›×™ ××•×§×“×
    'last_date': [topic_df[topic_df['topic_id'] == t]['published_date'].max().date() for t in large_topics],    # ×ª××¨×™×š ××—×¨×•×Ÿ ××—×¤×© ××ª ×”×ª××¨×™×š ×”×›×™ ×××•×—×¨
    'main_country': [topic_df[topic_df['topic_id'] == t]['rss_source'].mode()[0] for t in large_topics], # ××“×™× ×” ×¢×™×§×¨×™×ª mode()[0] ××—×–×™×¨ ××ª ×”××“×™× ×” ×”×›×™ ×©×›×™×—×”
    'article_ids': [topic_article_ids[t] for t in large_topics], # ×¨×©×™××ª ×›×ª×‘×•×ª ×‘×›×œ × ×•×©×
})

# -------------------------------------------------------------------------------------------
#  ğŸ”¹5. Printing examples from subject groups (up to 5)
# -------------------------------------------------------------------------------------------

print("\n=== ×“×•×’×××•×ª ×œ× ×•×©××™× ×¢× ×œ×¤×—×•×ª 5 ×›×ª×‘×•×ª ===")
for topic in large_topics:
    subset = topic_df[topic_df['topic_id'] == topic].sort_values('published_date') #subset = ×˜×‘×œ×” ×§×˜× ×” ×¢× ×›×œ ×”×›×ª×‘×•×ª ×©×œ ×”× ×•×©× ×”×–×”.  ××•×¦×™× ××ª ×›×œ ×”×›×ª×‘×•×ª ×©×©×™×™×›×•×ª ×œ× ×•×©× ×”×–×” (topic_id = topic) ×•×××™×™×Ÿ ××•×ª×Ÿ ×œ×¤×™ ×ª××¨×™×š ×¤×¨×¡×•×.
    print(f"\nğŸŸ¨ Topic ID: {topic} - {len(subset)} ×›×ª×‘×•×ª") # ××“×¤×™×¡ ××™×–×” × ×•×©× ××ª×” ××¦×™×’, ×•×›××” ×›×ª×‘×•×ª ×™×© ×‘×•.
    for _, row in subset.iterrows(): # ×¢×•×‘×¨ ×¢×œ ×›×œ ×©×•×¨×” (×›×ª×‘×”) ×‘×ª×ª-×”×˜×‘×œ×” subset.
        print(f"  - {row['published_date'].date()} | {row['rss_source']} | {row['title'][:70]}...")

# -------------------------------------------------------------------------------------------
#  ğŸ”¹6. Graphs - Trend development by number of articles per day (including moving average)
# -------------------------------------------------------------------------------------------

for topic in large_topics:#×¢×•×‘×¨ ×¢×œ ×›×œ × ×•×©× ×©××›×™×œ ×œ×¤×—×•×ª 5 ×›×ª×‘×•×ª
    subset = topic_df[topic_df['topic_id'] == topic] # subset = ×˜×‘×œ×” ×§×˜× ×” ×¢× ×›×œ ×”×›×ª×‘×•×ª ×©×œ ×”× ×•×©× ×”×–×”.
    topic_dates = subset['published_date'].value_counts().sort_index() # ××•× ×” ××ª ××¡×¤×¨ ×”×›×ª×‘×•×ª ×‘×›×œ ×ª××¨×™×š ×•×××™×™×Ÿ ×œ×¤×™ ×ª××¨×™×š

    # ×ª× ××™ ×¡×™× ×•×Ÿ â€“ ×¨×§ ×× ×™×© ×œ×¤×—×•×ª 3 ×™××™× ×©×•× ×™× ×•-5 ×›×ª×‘×•×ª ×œ×¤×—×•×ª
    if len(topic_dates) >= 3 and len(subset) >= 5: #×”×× ×™×© ×œ×¤×—×•×ª ×©× ×™ ×ª××¨×™×›×™× ×©×•× ×™×
        ma = topic_dates.rolling(window=2).mean() # ×—×™×©×•×‘ ×××•×¦×¢ × ×¢ ×©×œ 2 ×™××™×
        topic_dates.index = topic_dates.index.strftime('%Y-%m-%d')
        ma.index = ma.index.strftime('%Y-%m-%d')

        plt.figure(figsize=(10, 5))
        plt.plot(topic_dates.index, topic_dates.values, marker='o', label='Actual')
        plt.plot(ma.index, ma.values, linestyle='--', label='Moving Average (2 days)')
        plt.title(f"Trend for Topic ID {topic} - {len(topic_dates)} Days")
        plt.xlabel("Date")
        plt.ylabel("Number of Articles")
        plt.legend()
        plt.grid(True)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

print(" ×–×™×”×•×™ ××©×›×•×œ×•×ª ×•× ×™×ª×•×— ×”×ª×¤×ª×—×•×ª ×˜×¨× ×“×™× ×”×•×©×œ××•.")


# ===========================================================================================
# ğŸ”¹Step 9: Create summary tables â€“ Topics, Articles, Trends
# ===========================================================================================
print("============================================ ×™×¦×™×¨×ª ×˜×‘×œ××•×ª ××¡×›××•×ª â€“ Topics, Articles, Trends ===============================================")

# ×¡×™×›×•× × ×•×©××™× ×¢× ×©×“×•×ª ×ª×™××•×¨×™×™×
topics_df = topic_df.groupby('topic_id').agg(
    num_articles=('ID', 'count'), # ××¡×¤×¨ ×›×ª×‘×•×ª ×‘×›×œ × ×•×©×
    first_date=('published_date', 'min'), # ×ª××¨×™×š ×¨××©×•×Ÿ
    last_date=('published_date', 'max'), # ×ª××¨×™×š ××—×¨×•×Ÿ
    main_country=('rss_source', lambda x: x.mode()[0] if not x.mode().empty else None), # ××“×™× ×” ×¢×™×§×¨×™×ª
    top_keywords=('cleaned_keywords', lambda x: ', '.join(pd.Series([kw for kws in x for kw in kws]).value_counts().head(5).index)) , # 5 ××™×œ×•×ª ××¤×ª×— ×¤×•×¤×•×œ×¨×™×•×ª
).reset_index()

# short_title: ×”×›×•×ª×¨×ª ×©×œ ×”×›×ª×‘×” ×”×›×™ ××™×™×¦×’×ª (×”×›×™ ×“×•××” ×œ××—×¨×•×ª)
representative_titles = (
    topic_df.loc[topic_df.groupby('topic_id')['similar_count'].idxmax()] # ××•×¦× ×œ×›×œ topic_id ××ª ×”×›×ª×‘×” ×¢× ×”×›×™ ×”×¨×‘×” ×›×ª×‘×•×ª ×“×•××•×ª ××œ×™×”
    .set_index('topic_id')['title']  #×‘×•× ×” ×¡×“×¨×”: ×œ×›×œ topic_id ×”×›×•×ª×¨×ª ×”×›×™ ××™×™×¦×’×ª.
)
topics_df['short_title'] = topics_df['topic_id'].map(representative_titles) #××•×¡×™×£ ××ª ×”×›×•×ª×¨×ª ×”××™×™×¦×’×ª ×œ×˜×‘×œ×ª ×”× ×•×©××™×.

# ×›××” ×™××™× ×©×•× ×™× ×”×•×¤×™×¢×• ×›×ª×‘×•×ª ×œ×›×œ × ×•×©×
topic_day_counts = topic_df.groupby('topic_id')['published_date'].nunique()
max_articles_per_day = (# ××§×¡×™××•× ×›×ª×‘×•×ª ×‘×™×•× ××—×“ ×œ×›×œ × ×•×©×
    topic_df.groupby(['topic_id', 'published_date']).size()  # ××•× ×” ××ª ××¡×¤×¨ ×”×›×ª×‘×•×ª ×‘×›×œ × ×•×©× ×œ×¤×™ ×ª××¨×™×š
    .groupby('topic_id').max() # ××§×¡×™××•× ×›×ª×‘×•×ª ×‘×™×•× ××—×“ ×œ×›×œ × ×•×©×
)

# ×ª× ××™× ×œ×–×™×”×•×™ ×˜×¨× ×“×™×
spike_condition = max_articles_per_day >= 4 #× ×•×©× ×©×”×™×” ×œ×• ×œ×¤×—×•×ª ×™×•× ××—×“ ×¢× 4 ×›×ª×‘×•×ª ×•××¢×œ×”
growth_condition = topic_day_counts >= 3 #× ×•×©× ×©×”×™×” ×œ×• ×œ×¤×—×•×ª 3 ×™××™× ×©×•× ×™× ×¢× ×›×ª×‘×•×ª

# ××™×¤×•×™ ×”×¢××•×“×•×ª ×”××—×•×©×‘×•×ª
topics_df['num_days'] = topics_df['topic_id'].map(topic_day_counts) #×‘×›××” ×™××™× ×©×•× ×™× ×”×•× ×”×•×¤×™×¢
topics_df['max_articles_per_day'] = topics_df['topic_id'].map(max_articles_per_day) #××§×¡×™××•× ×›×ª×‘×•×ª ×‘×™×•× ××—×“ ×œ×›×œ × ×•×©×
topics_df['spike_detected'] = topics_df['topic_id'].map(spike_condition) #× ×•×©× ×©×”×™×” ×œ×• ×œ×¤×—×•×ª ×™×•× ××—×“ ×¢× 4 ×›×ª×‘×•×ª ×•××¢×œ×”
topics_df['growth_detected'] = topics_df['topic_id'].map(growth_condition) #× ×•×©× ×©×”×™×” ×œ×• ×œ×¤×—×•×ª 3 ×™××™× ×©×•× ×™× ×¢× ×›×ª×‘×•×ª

#  ×¡×™×•×•×’ ×¡×•×’ ×”×˜×¨× ×“ (Both / Spike / Growth / None)
def classify_trend(spike, growth):
    if spike and growth:
        return 'Both'
    elif spike:
        return 'Spike'
    elif growth:
        return 'Growth'
    else:
        return 'None'

topics_df['trend_type'] = topics_df.apply(
    lambda row: classify_trend(row['spike_detected'], row['growth_detected']),
    axis=1
)

#  dominant_day: ×”×™×•× ×©×‘×• ×”×ª×¨×—×© ×”×©×™× ×¢×‘×•×¨ ×›×œ topic
dominant_day = (
    topic_df.groupby(['topic_id', 'published_date']).size() #×¡×•×¤×¨ ×›××” ×›×ª×‘×•×ª ×”×™×• ×‘×›×œ ×™×•× ×œ×›×œ × ×•×©×.
    .groupby('topic_id').idxmax() #××•×¦× ××ª ×”×ª××¨×™×š ×©×‘×• ×”×™×” ×”×›×™ ×”×¨×‘×” ×›×ª×‘×•×ª ×œ×›×œ × ×•×©×
    .apply(lambda x: x[1]) #×œ×•×§×— ×¨×§ ××ª ×”×ª××¨×™×š (x[1] ×›×™ ×–×” ×˜×•×¤×œ ×›Ö¾tuple), ×•×××™×¨ ×œ×¤×•×¨××˜ ×™×¤×” YYYY-MM-DD.
)
topics_df['dominant_day'] = topics_df['topic_id'].map(dominant_day)

# ×¡×“×¨ ×¢××•×“×•×ª ×¡×•×¤×™
topics_df = topics_df[[
    'topic_id', 'short_title', 'trend_type',
    'num_articles', 'num_days', 'max_articles_per_day','first_date','dominant_day', 'last_date', 'main_country', 'top_keywords',
    'spike_detected', 'growth_detected'
]]

print("\n ×“×•×’××” ×œ×˜×‘×œ×ª Topics:")
print(topics_df.head())

# -------------------------------------------------------------------------------------------
#  ğŸ”¹2. Building an Articles table â€“ articles with association to topics
# -------------------------------------------------------------------------------------------

# ×¨×§ ×›×ª×‘×•×ª ×©××©×•×™×›×•×ª ×œ× ×•×©××™×
articles_df = topic_df[[
    'ID', 'title', 'summary', 'url',
    'published_date', 'published_time', 'rss_source',
    'cleaned_keywords', 'text_hash', 'topic_id', 'similar_count'
]].copy()

articles_df.rename(columns={'ID': 'article_id'}, inplace=True)# ×©×™× ×•×™ ×©× ×”×¢××•×“×” ×œ-article_id

# × ×™×§×•×™ cleaned_keywords ×œ×¨×©×™××” ××•×¤×¨×“×ª ×‘×¤×¡×™×§×™×
articles_df['cleaned_keywords'] = articles_df['cleaned_keywords'].apply(
    lambda kws: ', '.join(kws) if isinstance(kws, list) else ''
)

print("\n ×“×•×’××” ×œ×˜×‘×œ×ª Articles:")
print(articles_df.head())

# -------------------------------------------------------------------------------------------
#  ğŸ”¹3. Build a Trends table â€“ only topics that are Spike / Growth / Both
# -------------------------------------------------------------------------------------------

trends_df = topics_df[topics_df['trend_type'] != 'None'].copy()
print("\n ×“×•×’××” ×œ×˜×‘×œ×ª Trends:")
print(trends_df.head())

# -------------------------------------------------------------------------------------------
#  ğŸ”¹4. Saving the tables to the database
# -------------------------------------------------------------------------------------------

topics_df.to_sql('Topics', con=engine, index=False, if_exists='replace')
articles_df.to_sql('Articles', con=engine, index=False, if_exists='replace')
trends_df.to_sql('Trends', con=engine, index=False, if_exists='replace')

print(f"\nâœ… Topics saved: {len(topics_df)}")
print(f"âœ… Articles saved: {len(articles_df)}")
print(f"âœ… Trends saved: {len(trends_df)}")


# ===========================================================================================
# ğŸ”¹ 10. Business visualization of trends
# ===========================================================================================
print("============================================ ×•×™×–×•××œ×™×–×¦×™×” ×¢×¡×§×™×ª ×©×œ ×˜×¨× ×“×™× ===============================================")

# -------------------------------------------------------------------------------------------
# ğŸ”¹Graph 1: Columns â€“ Top 10 trends by number of articles
# -------------------------------------------------------------------------------------------

top_trends = trends_df.sort_values(['num_articles', 'num_days'], ascending=False).head(10)
top_topic_ids = top_trends['topic_id'].tolist() #×¨×©×™××ª ××–×”×™ topic (topic_id) ×¢×‘×•×¨ ×”×’×¨×¤×™×
id_to_title = dict(zip(top_trends['topic_id'], top_trends['short_title'])) #××™×œ×•×Ÿ ×©×××¤×” ××–×”×” ×œ× ×•×©× ×§×¦×¨ ×œ×”×¦×’×” (short_title) ×©×™××•×©×™ ×‘×ª×•×•×™×•×ª ×©×œ ×”×’×¨×¤×™×.

plt.figure(figsize=(10, 6))
sns.barplot(
    data=top_trends,
    x='short_title',
    y='num_articles',
    hue='short_title',
    palette='Blues_d',
    legend=False
)
plt.title('Top 10 Trending Topics by Number of Articles')
plt.xlabel('Topic')
plt.ylabel('Number of Articles')
plt.xticks(rotation=15, ha='right')
plt.grid(axis='y')
plt.tight_layout()
plt.show()


# -------------------------------------------------------------------------------------------
# ğŸ”¹ Graph 2: Lines â€“ daily development of 10 leading trends
# -------------------------------------------------------------------------------------------

plt.figure(figsize=(12, 7))
for topic_id in top_topic_ids:
    short_title = id_to_title[topic_id]
    subset = topic_df[topic_df['topic_id'] == topic_id]
    counts = subset['published_date'].value_counts().sort_index()
    plt.plot(counts.index, counts.values, marker='o', label=short_title)

plt.title('Daily Article Count for Top Trending Topics')
plt.xlabel('Date')
plt.ylabel('Number of Articles')
plt.legend(title='Topics')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

# -------------------------------------------------------------------------------------------
# ğŸ”¹Printing 10 sample articles from each leading trend
# -------------------------------------------------------------------------------------------
for topic_id in top_topic_ids: #×¢×•×‘×¨ ×¢×œ ×›×œ topic_id ×©× ××¦× ×‘×¨×©×™××ª top_topic_ids, ×©× ×‘×—×¨×• ×§×•×“× ×›Ö¾Top 10 ×˜×¨× ×“×™×.
    short_title = id_to_title[topic_id] #××•×¦× ××ª ×”×›×•×ª×¨×ª ×”×§×¦×¨×” ×©×œ ×”× ×•×©× ×”×–×” (short_title) ××ª×•×š ×”××™×œ×•×Ÿ ×©×”×›× ×• ×§×•×“×.
    print(f"\nğŸ”¹ {short_title} (Topic ID: {topic_id})\n") #××“×¤×™×¡ ×›×•×ª×¨×ª ×‘×¨×•×¨×” ×œ××¡×š, ×›×“×™ ×œ×”×‘×—×™×Ÿ ×‘×™×Ÿ × ×•×©××™× â€” ×›×•×œ×œ ×”×©× ×”××§×•×¦×¨ ×©×œ ×”Ö¾topic ×•×”Ö¾ID ×©×œ×•.

    print(
        topic_df[topic_df['topic_id'] == topic_id][[ #××¡× ×Ÿ ×¨×§ ××ª ×”×›×ª×‘×•×ª ×©×©×™×™×›×•×ª ×œ× ×•×©× ×”× ×•×›×—×™.
             'title','published_date','cleaned_keywords', 'rss_source' #×‘×•×—×¨ ×¨×§ ××ª ×”×¢××•×“×•×ª ×”×¨×œ×•×•× ×˜×™×•×ª ×œ×ª×¦×•×’×”.
        ]].sort_values('published_date').head(10) #×××™×™×Ÿ ×œ×¤×™ ×ª××¨×™×š.
    )

# -------------------------------------------------------------------------------------------
# ğŸ”¹Graph 3: WordCloud of keywords from trends
# -------------------------------------------------------------------------------------------
# ×—×™×‘×•×¨ ×›×œ ××™×œ×•×ª ×”××¤×ª×— ×©×œ ×˜×¨× ×“×™× ×œÖ¾String ××—×“
trend_keywords_series = trends_df['top_keywords'].dropna().astype(str)
text_blob = ' '.join(trend_keywords_series)

# ×™×¦×™×¨×ª WordCloud
wordcloud = WordCloud(width=1000, height=600, background_color='white', colormap='Blues').generate(text_blob)

# ×ª×¦×•×’×”
plt.figure(figsize=(12, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Top Keywords in Trending Topics (WordCloud)', fontsize=16)
plt.tight_layout()
plt.show()

# -------------------------------------------------------------------------------------------
# ğŸ”¹Graph 4: Horizontal bar graph â€“ Top 5 trends with representative summary
# -------------------------------------------------------------------------------------------
top_topics = trends_df.sort_values('num_articles', ascending=False).head(5)['topic_id'] # 5 × ×•×©××™× ×¢× ×”×›×™ ×”×¨×‘×” ×›×ª×‘×•×ª
top_articles = articles_df[articles_df['topic_id'].isin(top_topics)] # ×‘×•×—×¨ ××ª ×›×œ ×”×›×ª×‘×•×ª ×©×©×™×™×›×•×ª ×œ××•×ª× 5 ×˜×¨× ×“×™×.

# ×××—×“ ××™×œ×•×ª ××¤×ª×— ×œ×¤×™ × ×•×©×
topic_keywords = (
    top_articles.groupby('topic_id')['cleaned_keywords']
    .apply(lambda x: ' '.join(x))
)

# ×”×•×¤×š ××ª ×”×˜×§×¡×˜×™× (×œ×¤×™ ×˜×¨× ×“) ×œÖ¾×•×§×˜×•×¨×™× ×©×œ ×¡×¤×™×¨×ª ××™×œ×™× (Bag of Words).
vec = CountVectorizer()
X = vec.fit_transform(topic_keywords)

# ××™×™×¦×¨ ××˜×¨×™×¦×” ×©×œ ××™×œ×•×ª ××¤×ª×— ×œ×¤×™ × ×•×©×
keyword_matrix = pd.DataFrame(X.toarray(), index=topic_keywords.index, columns=vec.get_feature_names_out())


# --- ×¤×•× ×§×¦×™×™×ª ×§×™×¦×•×¨ ×ª×§×¦×™×¨ ×œ×¢×“ 7 ×©×•×¨×•×ª ×©×œ ×›-100 ×ª×•×•×™× ---
def wrap_summary(text, max_len=100, max_lines=7):
    words = text.split()
    lines = []
    line = ""
    for word in words:
        if len(line + " " + word) <= max_len:
            line += " " + word
        else:
            lines.append(line.strip())
            line = word
            if len(lines) == max_lines:
                break
    if line and len(lines) < max_lines:
        lines.append(line.strip())
    return '\n'.join(lines)

# --- ×©×œ×‘ 1: ××™×¤×•×™ ×”×ª×§×¦×™×¨ ×”×›×™ ××™×™×¦×’ ×œ×›×œ ×˜×¨× ×“ ×œ×¤×™ ×”×›×™ ×”×¨×‘×” similar_count ---
topic_summary_map = (
    articles_df
    .sort_values(['topic_id', 'similar_count'], ascending=[True, False])
    .drop_duplicates('topic_id')
    .set_index('topic_id')['summary']
)

# --- ×©×œ×‘ 2: ×”×•×¡×¤×ª ×”×ª×§×¦×™×¨ ×œÖ¾trends_df ---
trends_df['representative_summary'] = trends_df['topic_id'].map(topic_summary_map)

# --- ×©×œ×‘ 3: ×‘×—×™×¨×ª 5 ×”×˜×¨× ×“×™× ×”×›×™ ×—×–×§×™× ---
top5 = trends_df.sort_values(by=['num_articles', 'num_days'], ascending=False).head(5).copy()

# --- ×©×œ×‘ 4: ×§×™×¦×•×¨ ×”×ª×§×¦×™×¨ ×œ×ª×¦×•×’×” ×‘×’×¨×£ ---
top5['wrapped_title'] = top5['representative_summary'].apply(wrap_summary)

# --- ×©×œ×‘ 5: ×™×¦×™×¨×ª ×ª×•×•×™×ª ×˜×§×¡×˜ ×¢× ×›×œ ×”××™×“×¢ ---
top5['label'] = top5.apply(lambda row: (
    f"Trend Type: {row['trend_type']}\n"
    f"Articles: {row['num_articles']} | Days: {row['num_days']}\n"
    f"Max per Day: {row['max_articles_per_day']}\n"
    f"Dominant Day: {row['dominant_day']}\n"
    f"Country: {row['main_country']}\n"
    f"Keywords: {row['top_keywords']}"
), axis=1)

# --- ×©×œ×‘ 6: ×¦×™×•×¨ ×’×¨×£ ××•×¤×§×™ ××¡×•×“×¨ ---
plt.figure(figsize=(14, 10))
bars = plt.barh(top5['wrapped_title'], top5['num_articles'], color='skyblue')

for bar, label in zip(bars, top5['label']):
    plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
             label, va='center', fontsize=9)

plt.title("Top 5 Cyber Trends â€“ Based on Most Representative Article Summary", fontsize=16)
plt.xlabel("Number of Articles")
plt.ylabel("Representative Summary (trimmed)")
plt.grid(axis='x')
plt.tight_layout(rect=[0, 0, 0.95, 1])  # ×”×¨×—×‘×ª ×”×©×•×œ×™×™× ×™××™× ×”
plt.show()

